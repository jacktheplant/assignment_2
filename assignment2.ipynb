{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d6705c7-11b8-4bc7-9348-f8dacbfc880a",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bafbe73e-114d-4267-b282-10a46b7920e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(PCG64) at 0x1086AD0E0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.default_rng(seed=2026)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cb4065b-4d32-49ba-b21f-71d13aa652b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "spam_df = pd.read_csv('spambase.data', header=None)\n",
    "# without the header=None flag, the first row ended up being the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a685cab6-b73f-46a6-8d82-68a047b642f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Data Preview: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9   ...    48  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.00   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "\n",
       "      49   50     51     52     53     54   55    56  57  \n",
       "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Spam Data Preview: \\n\")\n",
    "spam_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36841b48-3cc7-4db5-8dd8-2a15629bb002",
   "metadata": {},
   "source": [
    "We still need the column names, which we can get from the file spambase.names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efd25e14-a468-4d52-b1e2-166e2f929261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list with each line in the file\n",
    "with open('spambase.names', 'r') as file:\n",
    "    file_lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fd74f15-5536-407f-ab2e-37b055f6230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the column names start at line 34\n",
    "col_name_lines = file_lines[33:]\n",
    "# for col_name in col_names: print(col_name) #- > this confirms that we are starting at the correct line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4abeb4-9342-4e7c-8a3c-4a3580ed4019",
   "metadata": {},
   "source": [
    "We also need to cut off everything after the colon in each line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa077366-a5ef-4946-bba8-162a77d31c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [col_name_line.split(':')[0] for col_name_line in col_name_lines]\n",
    "# for col_name in col_names: print(col_name) # -> this confirms the success of the parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f5b0277-431f-4e86-b267-32c8bdbc426a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in dataset: 58\n",
      "Number of column names acquired: 57\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of columns in dataset: {spam_df.shape[1]}\")\n",
    "print(f\"Number of column names acquired: {len(col_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eae5e22-85ef-4f52-83a2-4ad48fe8f5f0",
   "metadata": {},
   "source": [
    "We are missing the final column, which is the label column (1 for spam, 0 for not spam). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eee55d0-17e0-42a8-be65-f38b26bda319",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names.append(\"is_spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d839ade-85c7-4f19-934c-74f089ee4f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, assign the column names\n",
    "spam_df.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ff03927-dd97-4446-ba3e-ee9f05a5591d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Data Preview: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>is_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "0             0.00            0.00  ...         0.00        0.000   \n",
       "1             0.00            0.94  ...         0.00        0.132   \n",
       "2             0.64            0.25  ...         0.01        0.143   \n",
       "3             0.31            0.63  ...         0.00        0.137   \n",
       "4             0.31            0.63  ...         0.00        0.135   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0          0.0        0.778        0.000        0.000   \n",
       "1          0.0        0.372        0.180        0.048   \n",
       "2          0.0        0.276        0.184        0.010   \n",
       "3          0.0        0.137        0.000        0.000   \n",
       "4          0.0        0.135        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  is_spam  \n",
       "0                       278        1  \n",
       "1                      1028        1  \n",
       "2                      2259        1  \n",
       "3                       191        1  \n",
       "4                       191        1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Spam Data Preview: \\n\")\n",
    "spam_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89c5dc42-6d93-46ae-af97-2528f1cb0804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of dataset: (4601, 58)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dimensions of dataset: {spam_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f285ed41",
   "metadata": {},
   "source": [
    "Before going any further, we want to make sure our data is clean and won't cause issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aad47410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "word_freq_make                0\n",
      "word_freq_address             0\n",
      "word_freq_all                 0\n",
      "word_freq_3d                  0\n",
      "word_freq_our                 0\n",
      "word_freq_over                0\n",
      "word_freq_remove              0\n",
      "word_freq_internet            0\n",
      "word_freq_order               0\n",
      "word_freq_mail                0\n",
      "word_freq_receive             0\n",
      "word_freq_will                0\n",
      "word_freq_people              0\n",
      "word_freq_report              0\n",
      "word_freq_addresses           0\n",
      "word_freq_free                0\n",
      "word_freq_business            0\n",
      "word_freq_email               0\n",
      "word_freq_you                 0\n",
      "word_freq_credit              0\n",
      "word_freq_your                0\n",
      "word_freq_font                0\n",
      "word_freq_000                 0\n",
      "word_freq_money               0\n",
      "word_freq_hp                  0\n",
      "word_freq_hpl                 0\n",
      "word_freq_george              0\n",
      "word_freq_650                 0\n",
      "word_freq_lab                 0\n",
      "word_freq_labs                0\n",
      "word_freq_telnet              0\n",
      "word_freq_857                 0\n",
      "word_freq_data                0\n",
      "word_freq_415                 0\n",
      "word_freq_85                  0\n",
      "word_freq_technology          0\n",
      "word_freq_1999                0\n",
      "word_freq_parts               0\n",
      "word_freq_pm                  0\n",
      "word_freq_direct              0\n",
      "word_freq_cs                  0\n",
      "word_freq_meeting             0\n",
      "word_freq_original            0\n",
      "word_freq_project             0\n",
      "word_freq_re                  0\n",
      "word_freq_edu                 0\n",
      "word_freq_table               0\n",
      "word_freq_conference          0\n",
      "char_freq_;                   0\n",
      "char_freq_(                   0\n",
      "char_freq_[                   0\n",
      "char_freq_!                   0\n",
      "char_freq_$                   0\n",
      "char_freq_#                   0\n",
      "capital_run_length_average    0\n",
      "capital_run_length_longest    0\n",
      "capital_run_length_total      0\n",
      "is_spam                       0\n",
      "dtype: int64\n",
      "\n",
      "Total number of missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Check if there are missing values in each column of our spam_df\n",
    "print(\"Missing values per column:\")\n",
    "print(spam_df.isnull().sum())\n",
    "\n",
    "# Total count of missing values in the dataframe\n",
    "print(f\"\\nTotal number of missing values: {spam_df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a24ebbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column types in spam_df:\n",
      "float64    55\n",
      "int64       3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Ensure all feature columns are numeric (ints/floats)\n",
    "print(f\"Column types in spam_df:\\n{spam_df.dtypes.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad3198a-2a7a-433e-ace8-c10009d31318",
   "metadata": {},
   "source": [
    "We see here that we indeed have 57 continuous features and 1 binary classifier. Now, we split the data into a test and train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa3f09cc-88eb-4a24-8882-87517339fa25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 230\n",
      "Number of testings samples: 4371\n"
     ]
    }
   ],
   "source": [
    "# Get the row indices of the dataframe \n",
    "samples = spam_df.index.tolist()   \n",
    "\n",
    "# shuffle\n",
    "np.random.shuffle(samples)\n",
    "\n",
    "# divide this with a 5% / 95% split\n",
    "train_ind = samples[:round(len(samples)*0.05)]\n",
    "test_ind = samples[-round(len(samples)*0.95):]\n",
    "\n",
    "print(f\"Number of training samples: {len(train_ind)}\")\n",
    "print(f\"Number of testings samples: {len(test_ind)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9583131-de11-44e6-bd33-bb7e163dcebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset\n",
    "\n",
    "# inputs only\n",
    "X = spam_df.iloc[:, :-1]\n",
    "\n",
    "# outputs only\n",
    "y = spam_df[\"is_spam\"]\n",
    "\n",
    "# train-test split\n",
    "X_train = X.iloc[train_ind]\n",
    "X_test = X.iloc[test_ind]\n",
    "\n",
    "y_train = y.iloc[train_ind]\n",
    "y_test = y.iloc[test_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab3d0f3",
   "metadata": {},
   "source": [
    "Here, we want to check the label distribution in our entire dataset and the training set to ensure the training set isn't too skewed, potentially introducing bias into our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c43a2b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution in total dataset: is_spam\n",
      "0    0.605955\n",
      "1    0.394045\n",
      "Name: proportion, dtype: float64\n",
      "Label distribution in Training Set: is_spam\n",
      "0    0.630435\n",
      "1    0.369565\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "label_dist_spam_df = spam_df[\"is_spam\"].value_counts(normalize=True)\n",
    "label_dist_y_train = y_train.value_counts(normalize=True)\n",
    "print(f\"Label distribution in total dataset: {label_dist_spam_df}\")\n",
    "print(f\"Label distribution in Training Set: {label_dist_y_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbab744f-dcc5-4811-969a-826b08c6addf",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "\n",
    "NEED TO EXPAND ON THIS SECTION (ADD MORE PRE-PROCESSING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0740b3c9-4acf-4145-a122-ef9953b977a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardization\n",
    "def standardize(df):\n",
    "    \"\"\"\n",
    "    Standardizes each column of a dataframe, scaling the column to a standard Normal distribution.\n",
    "\n",
    "    Parameters:\n",
    "    df: the Pandas DataFrame to be standardized\n",
    "    \n",
    "    Returns: \n",
    "    standard_df: the standardized DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    standard_df = (df_copy - df_copy.mean()) / df_copy.std()\n",
    "\n",
    "    return standard_df\n",
    "\n",
    "\n",
    "X_train = standardize(X_train)\n",
    "X_test = standardize(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf2db2-d48e-4106-a238-b812d50198e8",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "Starting with the code provided in the Logistic Regression Code Review, but implemented changes as needed for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15769e94-ad2c-4739-9bf0-4755cdd0980c",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "Here, the cost function changes as we need to include L2 Regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a46cfef-38fb-482f-a2f9-9bc0d92c97e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use \"lambda_\" as lambda is a function already\n",
    "def cost_fn(x, y, w, lambda_=1e-3):\n",
    "    \"\"\"\n",
    "    Compute the binary cross-entropy cost (negative log-likelihood).\n",
    "    \n",
    "    Parameters:\n",
    "    x: input features, shape (N, D)\n",
    "    y: binary labels (0 or 1), shape (N,)\n",
    "    w: weights, shape (D,)\n",
    "    lamba_: regularization strength\n",
    "\n",
    "    Note: input parameters are a subset of the total dataset (mini-batch)\n",
    "    \n",
    "    Returns:\n",
    "    J: scalar cost value (lower is better)\n",
    "    \"\"\"\n",
    "    N, D = x.shape\n",
    "    \n",
    "    # Compute logits: z = x @ w, shape (N,)\n",
    "    # These are the raw predictions before applying sigmoid\n",
    "    z = np.dot(x, w)\n",
    "    \n",
    "    # Binary cross-entropy loss (mean over all samples in mini-batch)\n",
    "    # np.log1p(x) computes log(1 + x) with better numerical stability\n",
    "    # For y=1: cost is log(1 + exp(-z)) which penalizes z being too negative\n",
    "    # For y=0: cost is log(1 + exp(z)) which penalizes z being too positive\n",
    "    J = np.mean(y * np.log1p(np.exp(-z)) + (1-y) * np.log1p(np.exp(z)))\n",
    "\n",
    "    # now, add regularization penalty \n",
    "    J += (lambda_ / (2 * N)) * np.sum(w[1:]**2) \n",
    "    # here, we are using the divide by two convention. We also divide by N to use the mean cost, \n",
    "    # to keep this consistent with the mean binary cross-entropy loss implemented in the code review\n",
    "    # we assume w[0] is a bias, so it is excluded here\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b4eee1-a39d-491b-b443-9e981a8e02ab",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "Here, we must modify gradient descent to be stochastic (using mini-batches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ca793ca-b389-4b4a-9b4c-66fea10d95aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(self, x, y):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the cost function with respect to weights.\n",
    "    \n",
    "    The gradient tells us how to adjust weights to reduce the cost.\n",
    "    \n",
    "    Parameters:\n",
    "    x: input features, shape (N, D)\n",
    "    y: true binary labels, shape (N,)\n",
    "    \n",
    "    Returns:\n",
    "    grad: gradient vector, shape (D,)\n",
    "    \"\"\"\n",
    "    N, D = x.shape\n",
    "    \n",
    "    # Compute predictions using current weights\n",
    "    # yh = σ(x @ w), shape (N,)\n",
    "    # These are predicted probabilities in range (0, 1)\n",
    "    yh = logistic(np.dot(x, self.w))\n",
    "    \n",
    "    # Compute gradient: x^T @ (yh - y) / N\n",
    "    # x.T is (D, N), (yh - y) is (N,)\n",
    "    # Result is (D,) - one gradient value per weight\n",
    "    # Division by N because we use mean (not sum) in the cost function\n",
    "    # Positive gradient means increasing that weight would increase cost\n",
    "    grad = np.dot(x.T, yh - y) / N\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f512f25-7781-4d3f-9169-3cddfbe1f7a6",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "This implementation of logistic regression utilizes a fixed number of epochs, has been modified to accommodate the new changes as well (SDG and L2 Regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ecde2b1-ea1d-4183-9ddf-fd00b873ecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, add_bias=True, batch_size=1, learning_rate=.1, regularization_strength=1e-3, \n",
    "                 epsilon=1e-4, max_iters=1e5, verbose=False):\n",
    "        \"\"\"\n",
    "        Initialize Logistic Regression classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        add_bias: if True, adds an intercept term to the model\n",
    "        batch_size: size of mini-batches used for SDG\n",
    "        learing_rate: step size for gradient descent (also called eta)\n",
    "        \n",
    "        epsilon: convergence threshold - stop when ||gradient|| < epsilon\n",
    "        max_iters: maximum number of gradient descent iterations\n",
    "        verbose: if True, print optimization progress\n",
    "        \"\"\"\n",
    "        self.add_bias = add_bias\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.max_iters = max_iters\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        \"\"\"\n",
    "        Fit the logistic regression model using gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "        x: input features, shape (N,) or (N, D)\n",
    "        y: binary labels (0 or 1), shape (N,)\n",
    "        \n",
    "        Returns:\n",
    "        self: fitted model\n",
    "        \"\"\"\n",
    "        # If x is 1D, convert to 2D column vector\n",
    "        # x[:, None] adds a new axis: (N,) -> (N, 1)\n",
    "        if x.ndim == 1:\n",
    "            x = x[:, None]\n",
    "        \n",
    "        if self.add_bias:\n",
    "            N = x.shape[0]\n",
    "            # Add bias feature: append a column of ones\n",
    "            # x goes from (N, D) to (N, D+1)\n",
    "            x = np.column_stack([x, np.ones(N)])\n",
    "        \n",
    "        N, D = x.shape\n",
    "        \n",
    "        # Initialize weights to zero\n",
    "        self.w = np.zeros(D)\n",
    "        \n",
    "        # Initialize gradient norm to infinity (ensures loop starts)\n",
    "        g = np.inf\n",
    "        \n",
    "        # Iteration counter\n",
    "        t = 0\n",
    "        \n",
    "        # Gradient descent loop\n",
    "        # Stop when: (1) gradient is small enough, OR (2) max iterations reached\n",
    "        while np.linalg.norm(g) > self.epsilon and t < self.max_iters:\n",
    "            # Compute gradient at current weights\n",
    "            g = self.gradient(x, y)\n",
    "            \n",
    "            # Update weights: move in opposite direction of gradient\n",
    "            # w_new = w_old - learning_rate * gradient\n",
    "            # This moves \"downhill\" on the cost surface\n",
    "            self.w = self.w - self.learning_rate * g\n",
    "            \n",
    "            t += 1\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f'terminated after {t} iterations, with norm of the gradient equal to {np.linalg.norm(g):.6f}')\n",
    "            print(f'the weight found: {self.w}')\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict class probabilities for input x.\n",
    "        \n",
    "        Parameters:\n",
    "        x: input features, shape (N,) or (N, D)\n",
    "        \n",
    "        Returns:\n",
    "        yh: predicted probabilities P(y=1|x), shape (N,)\n",
    "        \"\"\"\n",
    "        # Convert 1D to 2D if needed\n",
    "        if x.ndim == 1:\n",
    "            x = x[:, None]\n",
    "        \n",
    "        Nt = x.shape[0]\n",
    "        \n",
    "        if self.add_bias:\n",
    "            # Add bias feature column\n",
    "            x = np.column_stack([x, np.ones(Nt)])\n",
    "        \n",
    "        # Compute probabilities: σ(x @ w)\n",
    "        # Output is in range (0, 1)\n",
    "        yh = logistic(np.dot(x, self.w))\n",
    "        \n",
    "        return yh\n",
    "\n",
    "# Attach the gradient method to the LogisticRegression class\n",
    "# This allows self.gradient(x, y) to work inside the fit method\n",
    "LogisticRegression.gradient = gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91542de8-601c-4a07-85e7-e91ee0f58102",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc34576-78ca-4c12-98eb-9dac16546df7",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b26b96c-d3f5-47b9-b693-bbef2bf3dbb4",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6492f7aa-d756-44b2-87ee-658c2e8c40fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
